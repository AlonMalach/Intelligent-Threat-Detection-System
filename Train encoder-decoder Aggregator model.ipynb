{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fe9002-11e6-42fa-85ac-9545e9995205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax, cross_entropy\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c69b1-7680-4539-acc2-3c9330af9e2d",
   "metadata": {},
   "source": [
    "# Matrix Bulid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cc8803-0ac3-41cc-90c4-e46b29e046ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"Cadets\" #Cadets, Theia\n",
    "data_folder = f\"dataset/{ds_name.lower()}/\"\n",
    "seq_len = 10 # set seq len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c18570",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Cadets_train_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Cadets_train_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b690e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"val\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Cadets_val_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Cadets_val_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6c0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"test\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Cadets_test_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Cadets_test_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51824481",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"Theia\" #Cadets, Theia\n",
    "data_folder = f\"dataset/{ds_name.lower()}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32dba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Theia_train_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Theia_train_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4c75ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"val\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Theia_val_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Theia_val_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a053b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"test\" #train, val, test\n",
    "\n",
    "fname_data_for_model = f\"{ds_name}_{set_name}_data_for_model_one_hot_types.pkl\"\n",
    "fname_data_for_graph = f\"{ds_name}_{set_name}_data_for_graph_one_hot_types.pkl\"\n",
    "\n",
    "\n",
    "Theia_test_data_for_model = pickle.load(open(data_folder+fname_data_for_model, \"rb\" ) )\n",
    "Theia_test_data_for_graph = pickle.load(open(data_folder+fname_data_for_graph, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c05ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df_cadets = pd.read_csv(\"dataset\\\\alerted_events.csv\")\n",
    "\n",
    "# train_df_cadets = full_df_cadets.iloc[:6511, :]\n",
    "# test_df_cadets = full_df_cadets.iloc[6511:, :]\n",
    "\n",
    "# full_df_theia = pd.read_csv(\"dataset\\\\alerted_events_Theia.csv\")\n",
    "# train_df_theia = full_df_theia.iloc[:2900, :]\n",
    "# test_df_theia = full_df_theia.iloc[2900:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eeb0067-8408-499e-b84d-1149786630a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict()\n",
    "options['input_size'] =  46 #len(train_logs[0][0])\n",
    "options['hidden_size'] = 16\n",
    "options['num_layers'] = 2\n",
    "options['num_classes'] = 6\n",
    "options['batch_size'] = 64\n",
    "options['p_dropout'] = 0.1\n",
    "options['bidirectional'] = False\n",
    "options['seq_len'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a033f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db362b7b-38a0-4698-abca-4c28227c1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab754f9-8f38-4375-814e-31ad20f4800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data#['data_for_sequences']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        log = torch.tensor(self.data[idx]['logs_for_sequence'], dtype=torch.long)\n",
    "        next_event_types = np.vstack(self.data[idx]['next_event_types']).astype(np.int)\n",
    "        next_event_types = torch.tensor(next_event_types, dtype=torch.long)\n",
    "        malicious_label = self.data[idx]['label']\n",
    "        return log, next_event_types, malicious_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64773bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of train sequences: 9400\n"
     ]
    }
   ],
   "source": [
    "both_train = Cadets_train_data_for_model['data_for_sequences'].copy()\n",
    "both_train.extend(Theia_train_data_for_model['data_for_sequences'])\n",
    "print('N of train sequences: {}'.format(len(both_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de72c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of test sequences: 750\n"
     ]
    }
   ],
   "source": [
    "both_test = Cadets_test_data_for_model['data_for_sequences'].copy()\n",
    "both_test.extend(Theia_test_data_for_model['data_for_sequences'])\n",
    "print('N of test sequences: {}'.format(len(both_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc39049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.from_numpy(Cadets_train_data_for_model['data_for_sequences'][0]['next_event_types'])\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # Your test array without 'dtype=object'\n",
    "# a = np.array([\n",
    "#     np.array([0.5, 1.0, 2.0], dtype=np.float16),\n",
    "#     np.array([4.0, 6.0, 8.0], dtype=np.float16),\n",
    "# ])\n",
    "\n",
    "# b = torch.from_numpy(a)\n",
    "\n",
    "# print(a.dtype) # This should not be 'object'\n",
    "# print(b)\n",
    "a = Cadets_train_data_for_model['data_for_sequences'][0]['next_event_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a72636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = log_dataset(both_train)\n",
    "train_loader = DataLoader(train_set,\n",
    "                               batch_size=1,#options['batch_size'],\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)\n",
    "\n",
    "\n",
    "test_set = log_dataset(both_test)\n",
    "test_loader = DataLoader(test_set,\n",
    "                               batch_size=1,#options['batch_size'],\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)\n",
    "\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# weights = compute_class_weight(class_weight='balanced',classes=np.unique(train_df['type']),y=train_df['type'])\n",
    "# weights = torch.Tensor(weights)\n",
    "# weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfa7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_example = next(iter(train_loader))[0]\n",
    "# input_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932a367-760e-48d2-bc24-05775261f0e8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386a0dc5-6fba-49fd-bde8-ce86204d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size=options['input_size'], hidden_size=options['hidden_size']):#p_dropout, bidirectional, num_layers, num_classes\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "#         self.bidirectional = bidirectional\n",
    "        self.hidden_size = input_size#hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.GRU(input_size=input_size,\n",
    "                            hidden_size=self.hidden_size,)\n",
    "#                             num_layers=num_layers,\n",
    "#                             batch_first=True,\n",
    "#                            dropout=p_dropout,\n",
    "#                            bidirectional=bidirectional)\n",
    "#         self.fc = nn.Linear(num_layers*2*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, one_log_event, hidden):\n",
    "#         embedded = self.embedding(one_log_event).view(1, 1, -1)\n",
    "        output = one_log_event.view(1, 1, -1).float()#embedded #\n",
    "#         print(one_log_event.shape, output.shape, hidden.shape)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "#         for log in logs:\n",
    "#             _, hid = self.lstm(log.float(), hid)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device) #1, num_layers, self.hidden_size\n",
    "    \n",
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=options['input_size'], output_size=options['num_classes'], \n",
    "                 dropout_p=options['p_dropout'], seg_length=options['seq_len']): #hidden_size=options['hidden_size']\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.encoder_out_to_output_size = nn.Linear(self.hidden_size, output_size)\n",
    "        self.attn = nn.Linear(hidden_size + output_size, seg_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + output_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "\n",
    "        self.out2 = nn.Linear(hidden_size//2, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        hidden = hidden.view(1, 1, -1)\n",
    "        input_embedded = input.view(1, 1, -1)#\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((input_embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((input_embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out1(output[0])\n",
    "        output = F.log_softmax(self.out2(output), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b56bafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM().to(device)\n",
    "decoder = AttnDecoderLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac05e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11544\\2809914774.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  next_event_types = np.vstack(self.data[idx]['next_event_types']).astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "          0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "          0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [5],\n",
       "         [5],\n",
       "         [5],\n",
       "         [2],\n",
       "         [5],\n",
       "         [5]]),\n",
       " 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40a9efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_example = next(iter(train_loader))[0]\n",
    "# input_example.squeeze().shape  #squeeze() because we have first dimension who is redundent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7de2216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #labels:\n",
    "# example_labels = next(iter(train_loader))[1]\n",
    "# example_labels.shape, example_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e06f82f8-269f-4e46-a207-ed9c20b7b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c61ed383-139b-4738-9702-a0d5d68016e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 #300 #\n",
    "# optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                             lr=0.0001,\n",
    "#                             betas=(0.9, 0.999))\n",
    "lr=0.0001\n",
    "criterion = nn.NLLLoss()#weight = torch.Tensor(weights).to(device))\n",
    "# val_criterion = torch.nn.CrossEntropyLoss()\n",
    "num_batch = len(train_loader)\n",
    "\n",
    "history = {'acc': [], 'epoch': [], 'loss': []}\n",
    "history_test = {'acc': [], 'epoch': [], 'loss': []}\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b2294",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78888b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_tok = torch.zeros(6, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ede19dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5 = torch.zeros(6, dtype=torch.long, device=device)\n",
    "a5[[3]] = 1\n",
    "a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c22d1fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11544\\2809914774.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  next_event_types = np.vstack(self.data[idx]['next_event_types']).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 15s (- 0m 0s) loss: 0.09636356657208585\n",
      "Mode:Test | Epoch 000: | Loss: 16.82724\n",
      "epoch 1/50:\n",
      "10m 38s (- 0m 0s) loss: 0.09351284534895207\n",
      "Mode:Test | Epoch 001: | Loss: 15.74095\n",
      "epoch 2/50:\n",
      "15m 39s (- 0m 0s) loss: 0.09079783341674597\n",
      "Mode:Test | Epoch 002: | Loss: 14.71578\n",
      "epoch 3/50:\n",
      "18m 52s (- 0m 0s) loss: 0.08860196740711695\n",
      "Mode:Test | Epoch 003: | Loss: 13.91779\n",
      "epoch 4/50:\n",
      "22m 20s (- 0m 0s) loss: 0.08634088917804408\n",
      "Mode:Test | Epoch 004: | Loss: 12.98874\n",
      "epoch 5/50:\n",
      "25m 44s (- 0m 0s) loss: 0.08384029366430179\n",
      "Mode:Test | Epoch 005: | Loss: 12.79223\n",
      "epoch 6/50:\n",
      "29m 9s (- 0m 0s) loss: 0.08240062626629176\n",
      "Mode:Test | Epoch 006: | Loss: 12.04663\n",
      "epoch 7/50:\n",
      "32m 33s (- 0m 0s) loss: 0.08007081454693388\n",
      "Mode:Test | Epoch 007: | Loss: 11.65128\n",
      "epoch 8/50:\n",
      "36m 2s (- 0m 0s) loss: 0.07837147076762034\n",
      "Mode:Test | Epoch 008: | Loss: 11.26966\n",
      "epoch 9/50:\n",
      "39m 29s (- 0m 0s) loss: 0.07689446571026448\n",
      "Mode:Test | Epoch 009: | Loss: 11.42342\n",
      "epoch 10/50:\n",
      "42m 55s (- 0m 0s) loss: 0.07559111116899589\n",
      "Mode:Test | Epoch 010: | Loss: 11.20826\n",
      "epoch 11/50:\n",
      "46m 26s (- 0m 0s) loss: 0.07430138356458735\n",
      "Mode:Test | Epoch 011: | Loss: 10.67705\n",
      "epoch 12/50:\n",
      "49m 55s (- 0m 0s) loss: 0.07316305477203637\n",
      "Mode:Test | Epoch 012: | Loss: 10.89023\n",
      "epoch 13/50:\n",
      "53m 20s (- 0m 0s) loss: 0.0717613279945956\n",
      "Mode:Test | Epoch 013: | Loss: 10.38600\n",
      "epoch 14/50:\n",
      "56m 46s (- 0m 0s) loss: 0.07094216557465191\n",
      "Mode:Test | Epoch 014: | Loss: 11.60791\n",
      "epoch 15/50:\n",
      "60m 14s (- 0m 0s) loss: 0.06911925822634253\n",
      "Mode:Test | Epoch 015: | Loss: 10.56140\n",
      "epoch 16/50:\n",
      "63m 43s (- 0m 0s) loss: 0.06791386659706976\n",
      "Mode:Test | Epoch 016: | Loss: 11.19412\n",
      "epoch 17/50:\n",
      "67m 8s (- 0m 0s) loss: 0.06759845884338316\n",
      "Mode:Test | Epoch 017: | Loss: 10.51244\n",
      "epoch 18/50:\n",
      "70m 35s (- 0m 0s) loss: 0.06718818319233652\n",
      "Mode:Test | Epoch 018: | Loss: 10.51164\n",
      "epoch 19/50:\n",
      "73m 59s (- 0m 0s) loss: 0.06622108505233358\n",
      "Mode:Test | Epoch 019: | Loss: 10.64005\n",
      "epoch 20/50:\n",
      "77m 54s (- 0m 0s) loss: 0.06442000930767292\n",
      "Mode:Test | Epoch 020: | Loss: 9.95106\n",
      "epoch 21/50:\n",
      "82m 35s (- 0m 0s) loss: 0.06354993502877808\n",
      "Mode:Test | Epoch 021: | Loss: 10.72039\n",
      "epoch 22/50:\n",
      "87m 48s (- 0m 0s) loss: 0.06293799514476583\n",
      "Mode:Test | Epoch 022: | Loss: 10.35615\n",
      "epoch 23/50:\n",
      "93m 12s (- 0m 0s) loss: 0.062405669056510016\n",
      "Mode:Test | Epoch 023: | Loss: 11.06404\n",
      "epoch 24/50:\n",
      "98m 39s (- 0m 0s) loss: 0.06063441697046001\n",
      "Mode:Test | Epoch 024: | Loss: 10.13895\n",
      "epoch 25/50:\n",
      "103m 52s (- 0m 0s) loss: 0.059466367347885234\n",
      "Mode:Test | Epoch 025: | Loss: 10.94445\n",
      "epoch 26/50:\n",
      "109m 14s (- 0m 0s) loss: 0.0615836581417635\n",
      "Mode:Test | Epoch 026: | Loss: 9.75378\n",
      "epoch 27/50:\n",
      "114m 31s (- 0m 0s) loss: 0.058710554149982\n",
      "Mode:Test | Epoch 027: | Loss: 9.99233\n",
      "epoch 28/50:\n",
      "548m 55s (- 0m 0s) loss: 0.057340063395848524\n",
      "Mode:Test | Epoch 028: | Loss: 10.21022\n",
      "epoch 29/50:\n",
      "552m 20s (- 0m 0s) loss: 0.05655041676163577\n",
      "Mode:Test | Epoch 029: | Loss: 10.06640\n",
      "epoch 30/50:\n",
      "555m 41s (- 0m 0s) loss: 0.05569581165942173\n",
      "Mode:Test | Epoch 030: | Loss: 8.83357\n",
      "epoch 31/50:\n",
      "559m 23s (- 0m 0s) loss: 0.055529317023704367\n",
      "Mode:Test | Epoch 031: | Loss: 10.06102\n",
      "epoch 32/50:\n",
      "562m 58s (- 0m 0s) loss: 0.05433977740099115\n",
      "Mode:Test | Epoch 032: | Loss: 9.99593\n",
      "epoch 33/50:\n",
      "566m 57s (- 0m 0s) loss: 0.05307238582790559\n",
      "Mode:Test | Epoch 033: | Loss: 8.97155\n",
      "epoch 34/50:\n",
      "570m 47s (- 0m 0s) loss: 0.052588732608105156\n",
      "Mode:Test | Epoch 034: | Loss: 10.23875\n",
      "epoch 35/50:\n",
      "574m 34s (- 0m 0s) loss: 0.052440612340690286\n",
      "Mode:Test | Epoch 035: | Loss: 8.70698\n",
      "epoch 36/50:\n",
      "578m 24s (- 0m 0s) loss: 0.05203714013006773\n",
      "Mode:Test | Epoch 036: | Loss: 9.15002\n",
      "epoch 37/50:\n",
      "581m 50s (- 0m 0s) loss: 0.05124139056023709\n",
      "Mode:Test | Epoch 037: | Loss: 11.61295\n",
      "epoch 38/50:\n",
      "585m 16s (- 0m 0s) loss: 0.05054130233755886\n",
      "Mode:Test | Epoch 038: | Loss: 10.57045\n",
      "epoch 39/50:\n",
      "588m 42s (- 0m 0s) loss: 0.05009164100470192\n",
      "Mode:Test | Epoch 039: | Loss: 8.07634\n",
      "epoch 40/50:\n",
      "592m 8s (- 0m 0s) loss: 0.04936876732281423\n",
      "Mode:Test | Epoch 040: | Loss: 8.72773\n",
      "epoch 41/50:\n",
      "595m 34s (- 0m 0s) loss: 0.049191242231992116\n",
      "Mode:Test | Epoch 041: | Loss: 9.35333\n",
      "epoch 42/50:\n",
      "599m 7s (- 0m 0s) loss: 0.04879361640588645\n",
      "Mode:Test | Epoch 042: | Loss: 11.46683\n",
      "epoch 43/50:\n",
      "602m 34s (- 0m 0s) loss: 0.047712733413542985\n",
      "Mode:Test | Epoch 043: | Loss: 9.90453\n",
      "epoch 44/50:\n",
      "606m 4s (- 0m 0s) loss: 0.046819779176642644\n",
      "Mode:Test | Epoch 044: | Loss: 10.72437\n",
      "epoch 45/50:\n",
      "609m 33s (- 0m 0s) loss: 0.04742806272173171\n",
      "Mode:Test | Epoch 045: | Loss: 10.19721\n",
      "epoch 46/50:\n",
      "613m 2s (- 0m 0s) loss: 0.046419144180601286\n",
      "Mode:Test | Epoch 046: | Loss: 10.12395\n",
      "epoch 47/50:\n",
      "616m 31s (- 0m 0s) loss: 0.04620061584720618\n",
      "Mode:Test | Epoch 047: | Loss: 11.71479\n",
      "epoch 48/50:\n",
      "619m 58s (- 0m 0s) loss: 0.04470883082789623\n",
      "Mode:Test | Epoch 048: | Loss: 9.89780\n",
      "epoch 49/50:\n",
      "623m 25s (- 0m 0s) loss: 0.04501237089347081\n",
      "Mode:Test | Epoch 049: | Loss: 11.72575\n"
     ]
    }
   ],
   "source": [
    "SOS_tok = torch.zeros(6, dtype=torch.long, device=device)\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "        \n",
    "for i in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    print(f'epoch {i}/{EPOCHS}:')\n",
    "    print_loss_total = 0 \n",
    "    # pbar = tqdm(enumerate(train_loader))\n",
    "    for idx, (train_logs, train_labels, malicious_label) in enumerate(train_loader):   \n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(options['seq_len'], encoder.hidden_size, device=device)\n",
    "        loss = 0\n",
    "        \n",
    "        for ei in train_logs.squeeze():\n",
    "            encoder_output, encoder_hidden = encoder(ei, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = SOS_tok\n",
    "        decoder_hidden = encoder_hidden\n",
    "        use_teacher_forcing = True\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for ind, di in enumerate(train_labels.squeeze()):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                loss += criterion(decoder_output.squeeze(), di)\n",
    "                decoder_input = torch.zeros(6, dtype=torch.long, device=device)  \n",
    "                decoder_input[di] = 1  # Teacher forcing\n",
    "        \n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for ind, di in enumerate(train_labels.squeeze()):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                loss += criterion(decoder_output.squeeze(), di)\n",
    "                \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        print_loss_total += loss.item() / options['seq_len']\n",
    "    \n",
    "    print(f\"{timeSince(start, 1)} loss: {print_loss_total/len(train_loader)}\")\n",
    "    \n",
    "    #validation:\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    with torch.no_grad():\n",
    "        for idx, (test_logs, test_labels, malicious_label) in enumerate(test_loader):\n",
    "#             output = model(test_logs).to(device)\n",
    "            encoder_outputs = torch.zeros(options['seq_len'], encoder.hidden_size, device=device)\n",
    "            loss = 0\n",
    "\n",
    "            for ei in test_logs.squeeze():\n",
    "                encoder_output, encoder_hidden = encoder(ei, encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = SOS_tok\n",
    "            decoder_hidden = encoder_hidden\n",
    "            use_teacher_forcing = True\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                for ind, di in enumerate(test_labels.squeeze()):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    loss += criterion(decoder_output.squeeze(), di)\n",
    "                    decoder_input = torch.zeros(6, dtype=torch.long, device=device)  \n",
    "                    decoder_input[di] = 1  # Teacher forcing\n",
    "\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                for ind, di in enumerate(test_labels.squeeze()):\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                    loss += criterion(decoder_output.squeeze(), di)\n",
    "\n",
    "            \n",
    "#             acc = (softmax(output, -1).argmax(-1) == label).float().sum()#categorical_accuracy(output, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "#             epoch_acc += acc.item()/options['seq_len']\n",
    "                # pbar.set_description(f\"{len(train_loader)}/{idx}\")\n",
    "    history_test['epoch'].append(i+1)\n",
    "    history_test['loss'].append(epoch_loss/(idx+1))\n",
    "#     history_test['acc'].append(epoch_acc/(idx+1))\n",
    "    print(f'Mode:Test | Epoch {i+0:03}: | Loss: {epoch_loss/len(test_loader):.5f}')# | Acc: {epoch_acc/(idx+1):.3f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eac4e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 10\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# time_of_saving = str(datetime.datetime.now())[:-13]\n",
    "# print(time_of_saving)\n",
    "# torch.save(encoder.state_dict(), f\"{time_of_saving} - encoder_model cadets and theia seq2seq2.pth\")\n",
    "# torch.save(decoder.state_dict(), f\"{time_of_saving} - decoder_model cadets and theia seq2seq2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0f853da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 5, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat((decoder_output.view(1,-1), decoder_output.view(1,-1)))\n",
    "torch.tensor([1,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6307c0d9-e969-4738-8c51-60fee00277ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for i in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     sum_loss = 0.0\n",
    "#     total = 0\n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "#     hid = None\n",
    "#     # pbar = tqdm(enumerate(train_loader))\n",
    "#     for idx, (train_logs, label, malicious_label) in enumerate(train_loader):      \n",
    "#         output = model(train_logs).to(device)\n",
    "#         # we transpose the predictions since crossentropy loss\n",
    "#         # expects to receive for the prediction a tensor shape (Batch, Classes, d1, d2, ....)\n",
    "#         # the output of the LSTM is shape (Batch, timesteps, classes)\n",
    "#         # so in order to have the correct format we need to transpose it to shape (Batch, classes, timesteps)\n",
    "#         # y is derived from batch which is a tensor of one-hot\n",
    "#         # crossentropy expects a categorical representation and not one-hot\n",
    "#         # so to extract the classes themselves we use argmax.\n",
    "#         print(output, label)\n",
    "#         loss = criterion(output.transpose(-1, -2), label)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         acc = (softmax(output, -1).argmax(-1) == label).float().sum()#categorical_accuracy(output, label)\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_acc += acc.item()/options['seq_len']\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         # pbar.set_description(f\"{len(train_loader)}/{idx}\")\n",
    "#     history['epoch'].append(i+1)\n",
    "#     history['loss'].append(epoch_loss/len(train_loader))\n",
    "#     history['acc'].append(epoch_acc/len(train_loader))\n",
    "#     print(f'Mode:Train | Epoch {i+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}%')            \n",
    "                   \n",
    "#     hid = None\n",
    "#     model.eval()\n",
    "#     sum_loss = 0.0\n",
    "#     total = 0\n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0 \n",
    "#     with torch.no_grad():\n",
    "#         for idx, (test_logs, label, malicious_label) in enumerate(test_loader):\n",
    "#             output = model(test_logs).to(device)\n",
    "\n",
    "#             loss = criterion(output.transpose(-1, -2), label)#, weight=weights\n",
    "#             acc = (softmax(output, -1).argmax(-1) == label).float().sum()#categorical_accuracy(output, label)\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#             epoch_acc += acc.item()/options['seq_len']\n",
    "#                 # pbar.set_description(f\"{len(train_loader)}/{idx}\")\n",
    "#     history_test['epoch'].append(i+1)\n",
    "#     history_test['loss'].append(epoch_loss/(idx+1))\n",
    "#     history_test['acc'].append(epoch_acc/(idx+1))\n",
    "#     print(f'Mode:Test | Epoch {i+0:03}: | Loss: {epoch_loss/len(test_loader):.5f} | Acc: {epoch_acc/(idx+1):.3f}%')\n",
    "\n",
    "#     # if i > 0 and history_test['loss'][i-1] < history_test['loss'][i]:\n",
    "#     #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6eecd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history['loss'], label='train loss')\n",
    "# plt.plot(history_test['loss'], label='validation loss')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6194bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history['acc'], label='train acc')\n",
    "# plt.plot(history_test['acc'], label='validation acc\"')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "158ab027-dddd-430e-961d-20547a10f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history['epoch'], history['acc'], label='accuracy')\n",
    "# plt.plot(history['epoch'], history['loss'], label='loss')\n",
    "# #plt.plot(validationhistory)\n",
    "# plt.title('Training')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331bafd",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28e96834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of val sequences: 210\n"
     ]
    }
   ],
   "source": [
    "both_val = Cadets_val_data_for_model['data_for_sequences'].copy()\n",
    "both_val.extend(Theia_val_data_for_model['data_for_sequences'])\n",
    "print('N of val sequences: {}'.format(len(both_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be38c6",
   "metadata": {},
   "source": [
    "## Find anomaly threshold from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43d4341b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18a4ae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|███████████████▌                                                                | 41/210 [00:00<00:00, 401.57it/s]\u001b[A\n",
      " 40%|████████████████████████████████▍                                               | 85/210 [00:00<00:00, 416.77it/s]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▌                              | 129/210 [00:00<00:00, 423.41it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 210/210 [00:00<00:00, 418.26it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# anomaly_score_ls = []\n",
    "mistakes_ls = []\n",
    "candidates = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq in tqdm(both_val):\n",
    "        ind = seq['seq_ind']\n",
    "        logs_input = torch.tensor([seq['logs_for_sequence']], dtype=torch.long).to(device)\n",
    "        next_event_types = torch.tensor(seq['next_event_types'], dtype=torch.long).long().to(device)\n",
    "        \n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(options['seq_len'], encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in logs_input.squeeze():\n",
    "            encoder_output, encoder_hidden = encoder(ei, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = SOS_tok\n",
    "        decoder_hidden = encoder_hidden\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "        mistakes = options['seq_len']\n",
    "#         print(labels.squeeze(), desired_i)\n",
    "\n",
    "        for ind, di in enumerate(next_event_types.squeeze()):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "#             print(decoder_output.topk(5)[0])\n",
    "            decoder_output = topi.squeeze().detach()  # detach from history as input\n",
    "            desired_i = di\n",
    "#             print(\"di\", di, \"desired_i\", desired_i)\n",
    "            if desired_i == decoder_output:\n",
    "                mistakes-=1\n",
    "            \n",
    "            decoder_input = torch.zeros(6, dtype=torch.long, device=device)  \n",
    "            if use_teacher_forcing: decoder_input[di] = 1  # Teacher forcing\n",
    "            else: decoder_input[decoder_output] = 1\n",
    "            \n",
    "        mistakes_ls.append(mistakes)\n",
    "        decoded_words.append(decoder_output)\n",
    "        \n",
    "        \n",
    "#         logs_input = torch.tensor([logs_input], dtype=torch.float).to(device)\n",
    "#         next_event_types = torch.tensor(next_event_types, dtype=torch.float).long().to(device)\n",
    "#         output = model(logs_input)\n",
    "        \n",
    "#         soft_out = softmax(output, dim=-1).squeeze()\n",
    "#         sorted_preds = torch.argsort(output.squeeze(), dim=-1 ,descending=True)\n",
    "#         anomaly_score = 0\n",
    "#         for ind_event, pred, correct_event_type in zip(range(seq_len), sorted_preds, next_event_types):\n",
    "#             top_k_preds = pred[:candidates] \n",
    "#             if correct_event_type not in top_k_preds: \n",
    "#                 anomaly_score += 0.2 * (1-soft_out[ind_event, correct_event_type])\n",
    "#         anomaly_score_ls.append(anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b00c7137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2857142857142857, 7)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(mistakes_ls), np.max(mistakes_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288d646",
   "metadata": {},
   "source": [
    "## Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ff81a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/750 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|████▉                                                                           | 46/750 [00:00<00:01, 446.61it/s]\u001b[A\n",
      " 12%|█████████▊                                                                      | 92/750 [00:00<00:01, 451.76it/s]\u001b[A\n",
      " 18%|██████████████▌                                                                | 138/750 [00:00<00:01, 449.39it/s]\u001b[A\n",
      " 24%|███████████████████▎                                                           | 183/750 [00:00<00:01, 442.74it/s]\u001b[A\n",
      " 30%|████████████████████████                                                       | 228/750 [00:00<00:01, 439.09it/s]\u001b[A\n",
      " 36%|████████████████████████████▋                                                  | 272/750 [00:00<00:01, 436.49it/s]\u001b[A\n",
      " 42%|█████████████████████████████████▎                                             | 316/750 [00:00<00:01, 427.01it/s]\u001b[A\n",
      " 48%|█████████████████████████████████████▊                                         | 359/750 [00:00<00:00, 421.42it/s]\u001b[A\n",
      " 54%|██████████████████████████████████████████▎                                    | 402/750 [00:00<00:00, 415.22it/s]\u001b[A\n",
      " 59%|██████████████████████████████████████████████▊                                | 444/750 [00:01<00:00, 397.70it/s]\u001b[A\n",
      " 65%|██████████████████████████████████████████████████▉                            | 484/750 [00:01<00:00, 387.19it/s]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████████                        | 523/750 [00:01<00:00, 378.25it/s]\u001b[A\n",
      " 75%|███████████████████████████████████████████████████████████                    | 561/750 [00:01<00:00, 363.22it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████▉                | 598/750 [00:01<00:00, 365.13it/s]\u001b[A\n",
      " 85%|███████████████████████████████████████████████████████████████████▏           | 638/750 [00:01<00:00, 374.04it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████████████████████████████████████▌       | 679/750 [00:01<00:00, 384.46it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 750/750 [00:01<00:00, 403.28it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "FP = 0\n",
    "FN = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "\n",
    "FP_ls = []\n",
    "TP_ls = []\n",
    "\n",
    "\n",
    "# anomaly_score_ls = []\n",
    "threshold_mistakes = 1\n",
    "mistakes_ls = []\n",
    "# candidates = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq in tqdm(both_test):\n",
    "        ind = seq['seq_ind']\n",
    "        logs_input = torch.tensor([seq['logs_for_sequence']], dtype=torch.long).to(device)\n",
    "        next_event_types = torch.tensor(seq['next_event_types'], dtype=torch.long).long().to(device)\n",
    "        malicious_label = seq['label']\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(options['seq_len'], encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in logs_input.squeeze():\n",
    "            encoder_output, encoder_hidden = encoder(ei, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "        decoder_input = SOS_tok\n",
    "        decoder_hidden = encoder_hidden\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "        mistakes = options['seq_len']\n",
    "#         print(labels.squeeze(), desired_i)\n",
    "\n",
    "        for ind, di in enumerate(next_event_types.squeeze()):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "#             print(decoder_output.topk(5)[0])\n",
    "            decoder_output = topi.squeeze().detach()  # detach from history as input\n",
    "            desired_i = di\n",
    "#             print(\"di\", di, \"desired_i\", desired_i)\n",
    "            if desired_i == decoder_output:\n",
    "                mistakes-=1\n",
    "            \n",
    "            decoder_input = torch.zeros(6, dtype=torch.long, device=device)  \n",
    "            if use_teacher_forcing: decoder_input[di] = 1  # Teacher forcing\n",
    "            else: decoder_input[decoder_output] = 1\n",
    "                \n",
    "        mistakes_ls.append(mistakes)\n",
    "        decoded_words.append(decoder_output)\n",
    "        \n",
    "\n",
    "        \n",
    "        if malicious_label == 1: # malicious\n",
    "            if mistakes<threshold_mistakes:\n",
    "                FN += 1\n",
    "#                 print(f\"FN: seq ind-{ind} mistakes:{mistakes}\")\n",
    "\n",
    "            else:\n",
    "#                 print(f\"TP: seq ind-{ind} mistakes:{mistakes}\")\n",
    "                TP_ls.append(ind)\n",
    "                TP += 1\n",
    "        elif malicious_label == 0: # benign\n",
    "            if mistakes<threshold_mistakes:\n",
    "                TN += 1\n",
    "#                 print(f\"TN: seq ind-{ind} mistakes:{mistakes}\")\n",
    "            else:\n",
    "#                 print(f\"FP: seq ind-{ind} mistakes:{mistakes}\")\n",
    "                FP_ls.append(ind)\n",
    "                FP += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa81afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive: 43, \n",
      "false positive (FP): 214, \n",
      "true negative: 493, \n",
      "false negative (FN): 0, \n",
      "Precision: 0.167, \n",
      "Recall: 1.000, \n",
      "False positive rate: 0.303\n"
     ]
    }
   ],
   "source": [
    "FPR = FP/(FP+TN)\n",
    "P = TP / (TP + FP) if TP + FP>0 else 0\n",
    "R = TP / (TP + FN) if TP + FN>0 else 0\n",
    "print('true positive: {}, \\nfalse positive (FP): {}, \\ntrue negative: {}, \\nfalse negative (FN): {}, \\nPrecision: {:.3f}, \\nRecall: {:.3f}, \\nFalse positive rate: {:.3f}'\n",
    "    .format(TP, FP, TN, FN, P, R, FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd6d2e",
   "metadata": {},
   "source": [
    "# Check metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9173e4",
   "metadata": {},
   "source": [
    "# second approach to anomaly - 0.2*softmax of truth event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "947a2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/750 [00:00<?, ?it/s]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11544\\2809914774.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  next_event_types = np.vstack(self.data[idx]['next_event_types']).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "# anomaly_labels = test_df['anomaly_label'][10:]\n",
    "hid = None\n",
    "\n",
    "threshold_anomaly_score = 0.76 #how many mistakes allowd in preds for a sequence to count as a benign \n",
    "seq_len = options['seq_len']\n",
    "candidates = 1\n",
    "\n",
    "pbar = tqdm(test_set, total=len(test_set), position=0, leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (test_logs, label, malicious_label) in enumerate(test_loader):   \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(options['seq_len'], encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in test_logs.squeeze():\n",
    "            encoder_output, encoder_hidden = encoder(ei, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "        SOS_tok = torch.zeros(1, options['input_size'], device=device)\n",
    "        decoder_input = SOS_tok\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        \n",
    "#     for seq_ind, sequence in enumerate(pbar):\n",
    "#         logs = sequence[0].unsqueeze(0) #add dimension to make it batch of one\n",
    "#         seq = torch.tensor(logs, dtype=torch.float).to(device)\n",
    "#         label = torch.tensor(sequence[1], dtype=torch.float).long().to(device)\n",
    "#         output = model(seq)\n",
    "# #         predicted = torch.argsort(output,1)[0][-candidates:]\n",
    "#         soft_out = softmax(output, dim=-1).squeeze()\n",
    "#         sorted_preds = torch.argsort(output.squeeze(), dim=-1 ,descending=True)#[:,:3] \n",
    "#         anomaly_score = 0\n",
    "#         for ind_event, pred, correct_event_type in zip(range(seq_len), sorted_preds, label):\n",
    "#             top_k_preds = pred[:candidates] \n",
    "#             if correct_event_type not in top_k_preds: \n",
    "#                 anomaly_score += 0.2 * (1-soft_out[ind_event, correct_event_type])\n",
    "# #         event_id_for_seq = event_ids_test[seq_ind]\n",
    "#         malicious_label = sequence[2]\n",
    "#         if malicious_label == 1: # anomaly\n",
    "#             #print(\"anomaly\")\n",
    "#             if anomaly_score<threshold_anomaly_score:\n",
    "#                 print(f\"seq ind - {seq_ind}: anomaly_score - {anomaly_score} -------FN-------- \")#event_ids - {event_id_for_seq}\")\n",
    "#                 FN += 1\n",
    "#             else:\n",
    "#                 print(f\"seq ind - {seq_ind}: anomaly_score - {anomaly_score} -------TP \")#event_ids - {event_id_for_seq}\")\n",
    "#                 TP += 1\n",
    "#         elif malicious_label == 0: # benign\n",
    "#             if anomaly_score<threshold_anomaly_score:\n",
    "#                 TN += 1\n",
    "#             else:\n",
    "#                 print(f\"seq ind - {seq_ind}: anomaly_score - {anomaly_score} FP------- \")#event_ids - {event_id_for_seq}\")\n",
    "#                 FP += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32c384f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mTP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m TP \u001b[38;5;241m/\u001b[39m (TP \u001b[38;5;241m+\u001b[39m FN)\n\u001b[0;32m      3\u001b[0m FPR \u001b[38;5;241m=\u001b[39m FP\u001b[38;5;241m/\u001b[39m(FP\u001b[38;5;241m+\u001b[39mTP)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "P = 100 * TP / (TP + FP)\n",
    "R = 100 * TP / (TP + FN)\n",
    "FPR = FP/(FP+TP)\n",
    "# F1 = 2 * P * R / (P + R)\n",
    "print('true positive: {}, \\nfalse positive (FP): {}, \\ntrue negative: {}, \\nfalse negative (FN): {}, \\nPrecision: {:.3f}%, \\nRecall: {:.3f}%, , \\nFalse positive rate: {:.3f}'\n",
    "    .format(TP, FP, TN, FN, P, R, FPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf319a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_closed_to_alerts = 22\n",
    "# new_FP = FP-fp_closed_to_alerts\n",
    "# FPR = new_FP/(new_FP+TP)\n",
    "# P = 100 * TP / (TP + new_FP) if TP + new_FP>0 else 0\n",
    "# R = 100 * TP / (TP + FN) if TP + FN>0 else 0\n",
    "# print('true positive: {}, \\nfalse positive (FP): {}, \\ntrue negative: {}, \\nfalse negative (FN): {}, \\nPrecision: {:.3f}%, \\nRecall: {:.3f}%, , \\nFalse positive rate: {:.3f}'\n",
    "#     .format(TP, new_FP, TN, FN, P, R, FPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# time_of_saving = str(datetime.datetime.now())[:-13]\n",
    "# print(time_of_saving)\n",
    "# torch.save(model.state_dict(), f\"{time_of_saving} - agregator_model cadets and theia 0.3 dropout.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
